frontend: embed     # embedding + positional encoding
frontend_conf:
    embed_dim: 512
    positional_dropout_rate: 0.3

encoder: transformer
encoder_conf:
    output_size: 256
    attention_heads: 4
    linear_units: 2048
    num_blocks: 8
    dropout_rate: 0.2
    input_layer: null
    normalize_before: true

token_decoder: linear


model_conf:
    token_loss_weight: 1.0
    lsm_weight: 0.1
    length_normalized_loss: false

use_amp: true       # automatic mixed precision (amp)
seed: 2022
log_interval: 100
num_workers: 2
batch_type: numel
batch_bins: 12000000
accum_grad: 1
max_epoch: 70
patience: none
init: none
best_model_criterion:
-   - valid
    - acc
    - max
-   - train
    - loss
    - min
keep_nbest_models: 10

optim: adam
optim_conf:
    lr: 0.00035
    weight_decay: 0.000001
scheduler: warmuplr
scheduler_conf:
    warmup_steps: 5000