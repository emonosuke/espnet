1,4c1,5
< # This configuration reuqires 3 gpus in the case of each gpu memory = 12GB, and it takes 2~3 days.
< # If you cannot use 3 gpus, it is better to make batch-bins small, increase accum_grad and epochs.
< # This model uses speaker emebdding with add integration, leading fast attention learning compared
< # to concat integration wihtout guided attention loss.
---
> # This configuration is for ESPnet2 to train Transformer-TTS. Compared to the
> # original paper, this configuration additionally use the guided attention
> # loss to accelerate the learning of the diagonal attention. It requires
> # 4 GPUs with 32 GB memory and it will finish within 1 day to finish the
> # training on Tesla V100.
6,33c7,54
< # network architecture related
< model-module: espnet.nets.pytorch_backend.e2e_tts_transformer:Transformer
< embed-dim: 0
< eprenet-conv-layers: 0  # one more linear layer w/o non-linear will be added for 0-centor
< eprenet-conv-filts: 0
< eprenet-conv-chans: 0
< dprenet-layers: 2  # one more linear layer w/o non-linear will be added for 0-centor
< dprenet-units: 256
< adim: 384
< aheads: 4
< elayers: 6
< eunits: 1536
< dlayers: 6
< dunits: 1536
< postnet-layers: 5
< postnet-filts: 5
< postnet-chans: 256
< use-masking: True
< bce-pos-weight: 5.0
< use-batch-norm: True
< use-scaled-pos-enc: True
< encoder-normalize-before: False
< decoder-normalize-before: False
< encoder-concat-after: False
< decoder-concat-after: False
< reduction-factor: 1
< use-speaker-embedding: true
< spk-embed-integration-type: add
---
> ##########################################################
> #                  TTS MODEL SETTING                     #
> ##########################################################
> tts: transformer           # model architecture
> tts_conf:                  # keyword arguments for the selected model
>     embed_dim: 0           # embedding dimension in encoder prenet
>     eprenet_conv_layers: 0 # number of conv layers in encoder prenet
>                            # if set to 0, no encoder prenet will be used
>     eprenet_conv_filts: 0  # filter size of conv layers in encoder prenet
>     eprenet_conv_chans: 0  # number of channels of conv layers in encoder prenet
>     dprenet_layers: 2      # number of layers in decoder prenet
>     dprenet_units: 128     # number of units in decoder prenet
>     adim: 256              # attention dimension
>     aheads: 4              # number of attention heads
>     elayers: 6             # number of encoder layers
>     eunits: 512           # number of encoder ff units
>     dlayers: 6             # number of decoder layers
>     dunits: 512           # number of decoder ff units
>     positionwise_layer_type: conv1d  # type of position-wise layer
>     positionwise_conv_kernel_size: 1 # kernel size of position wise conv layer
>     postnet_layers: 5                # number of layers of postnset
>     postnet_filts: 5                 # filter size of conv layers in postnet
>     postnet_chans: 128               # number of channels of conv layers in postnet
>     use_masking: True                # whether to apply masking for padded part in loss calculation
>     bce_pos_weight: 5.0              # weight of positive sample in binary cross entropy calculation
>     use_scaled_pos_enc: True         # whether to use scaled positional encoding
>     encoder_normalize_before: True   # whether to perform layer normalization before the input
>     decoder_normalize_before: True   # whether to perform layer normalization before the input
>     reduction_factor: 1              # reduction factor
>     init_type: xavier_uniform        # initialization type
>     init_enc_alpha: 1.0              # initial value of alpha of encoder scaled position encoding
>     init_dec_alpha: 1.0              # initial value of alpha of decoder scaled position encoding
>     eprenet_dropout_rate: 0.0        # dropout rate for encoder prenet
>     dprenet_dropout_rate: 0.5        # dropout rate for decoder prenet
>     postnet_dropout_rate: 0.5        # dropout rate for postnet
>     transformer_enc_dropout_rate: 0.1                # dropout rate for transformer encoder layer
>     transformer_enc_positional_dropout_rate: 0.1     # dropout rate for transformer encoder positional encoding
>     transformer_enc_attn_dropout_rate: 0.1           # dropout rate for transformer encoder attention layer
>     transformer_dec_dropout_rate: 0.1                # dropout rate for transformer decoder layer
>     transformer_dec_positional_dropout_rate: 0.1     # dropout rate for transformer decoder positional encoding
>     transformer_dec_attn_dropout_rate: 0.1           # dropout rate for transformer decoder attention layer
>     transformer_enc_dec_attn_dropout_rate: 0.1       # dropout rate for transformer encoder-decoder attention layer
>     use_guided_attn_loss: true                       # whether to use guided attention loss
>     num_heads_applied_guided_attn: 2                 # number of layers to apply guided attention loss
>     num_layers_applied_guided_attn: 2                # number of heads to apply guided attention loss
>     modules_applied_guided_attn: ["encoder-decoder"] # modules to apply guided attention loss
>     guided_attn_loss_sigma: 0.4                      # sigma in guided attention loss
>     guided_attn_loss_lambda: 10.0                    # lambda in guided attention loss
35,38c56,65
< # minibatch related
< batch-sort-key: output # shuffle or input or output
< batch-bins: 1500000   # batch-size * (max_out * dim_out + max_in * dim_in)
<                       # 5203 batches containing from 5 to 354 samples (avg 28 samples).
---
> ##########################################################
> #            OPTIMIZER & SCHEDULER SETTING               #
> ##########################################################
> optim: adam             # optimizer type
> optim_conf:             # keyword arguments for selected optimizer
>     lr: 1.0             # learning rate
> scheduler: noamlr       # scheduler type
> scheduler_conf:         # keyword arguments for selected scheduler
>     model_size: 512     # model size, a.k.a., attention dimenstion
>     warmup_steps: 8000  # the number of warmup steps
40,68c67,91
< # training related
< transformer-init: pytorch
< transformer-warmup-steps: 10000
< transformer-lr: 1.0
< initial-encoder-alpha: 1.0
< initial-decoder-alpha: 1.0
< eprenet-dropout-rate: 0.0
< dprenet-dropout-rate: 0.5
< postnet-dropout-rate: 0.5
< transformer-enc-dropout-rate: 0.1
< transformer-enc-positional-dropout-rate: 0.1
< transformer-enc-attn-dropout-rate: 0.1
< transformer-dec-dropout-rate: 0.1
< transformer-dec-positional-dropout-rate: 0.1
< transformer-dec-attn-dropout-rate: 0.1
< transformer-enc-dec-attn-dropout-rate: 0.1
< use-guided-attn-loss: false
< bce-pos-weight: 10.0
< 
< # optimization related
< opt: noam
< accum-grad: 4
< grad-clip: 1.0
< weight-decay: 0.0
< patience: 0
< epochs: 100 # 100 * 5,203 / 4 = 130,075 iters
< 
< # other
< save-interval-epoch: 1
\ No newline at end of file
---
> ##########################################################
> #                OTHER TRAINING SETTING                  #
> ##########################################################
> num_iters_per_epoch: 1000   # number of iterations per epoch
> max_epoch: 200              # number of epochs
> grad_clip: 1.0              # gradient clipping norm
> grad_noise: false           # whether to use gradient noise injection
> accum_grad: 2               # gradient accumulation
> batch_bins: 9000000         # batch bins (for feats_type=raw, *= n_shift / n_mels)
> batch_type: numel           # how to make batch
> sort_in_batch: descending   # how to sort data in making batch
> sort_batch: descending      # how to sort created batches
> num_workers: 1              # number of workers of data loader
> train_dtype: float32        # dtype in training
> log_interval: null          # log interval in iterations
> keep_nbest_models: 5        # number of models to keep
> num_att_plot: 3             # number of attention figures to be saved in every check
> seed: 0                     # random seed number
> best_model_criterion:
> -   - valid
>     - loss
>     - min
> -   - train
>     - loss
>     - min
